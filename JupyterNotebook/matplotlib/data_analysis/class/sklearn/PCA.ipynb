{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_DF = pd.read_csv(\"./winequality.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = wine_DF.values[:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一: 数学降维\n",
    "# 1. 标准化\n",
    "sc = StandardScaler()\n",
    "wine_std = sc.fit_transform(wine_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "xxx\n",
      "(12, 12)\n"
     ]
    }
   ],
   "source": [
    "# 2. 对标准化数据转置, 构建协方差矩阵\n",
    "cov_mat = np.cov(wine_std.T)\n",
    "\n",
    "# 3. 获取特征值和特征向量\n",
    "eifen_values, eigen_vectors = np.linalg.eig(cov_mat)\n",
    "print(eifen_values.shape)\n",
    "print(\"xxx\")\n",
    "print(eigen_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# 4. 特征值总和\n",
    "total = sum(eifen_values)\n",
    "# 特征值比例\n",
    "ration = [i/total for i in eifen_values]\n",
    "print(len(ration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 12)\n",
      "(12, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.36564764,  0.4720026 ,  1.11031457,  1.00504191,  0.15417471],\n",
       "       [-0.05018749,  0.79750082,  1.95384943, -0.09032724, -0.05366988],\n",
       "       [ 0.3535635 ,  0.24401092,  1.30234563,  0.22169751,  0.09752688],\n",
       "       [-3.16418264, -0.97329081, -1.94979796,  1.81342354, -0.57818242],\n",
       "       [ 1.36564764,  0.4720026 ,  1.11031457,  1.00504191,  0.15417471],\n",
       "       [ 1.22267158,  0.54560843,  0.97070229,  0.95371402, -0.01696395],\n",
       "       [ 0.64947792,  0.00972282,  0.65375281,  0.7407173 , -1.07737445],\n",
       "       [ 2.01105668, -0.79434676, -0.56069188,  1.35718334,  0.32142611],\n",
       "       [ 0.78477238, -0.77431235, -0.19024306,  2.22953694,  0.9573147 ],\n",
       "       [-1.205167  ,  2.03026079, -1.08577236, -0.93325341,  0.69743525],\n",
       "       [ 0.80955264, -0.21375519,  0.72273863, -0.04552891, -1.17587519],\n",
       "       [-1.205167  ,  2.03026079, -1.08577236, -0.93325341,  0.69743525],\n",
       "       [ 3.03646666,  0.37932868, -0.03296844, -1.0293463 , -0.85518884],\n",
       "       [-1.441007  , -1.95763858,  1.14463912, -1.06944519,  1.27061425],\n",
       "       [-4.05930022,  2.15471115,  1.64611132, -0.81860366, -0.96983209],\n",
       "       [-4.1007975 ,  2.17559175,  1.59842249, -0.90799786, -0.8912374 ],\n",
       "       [-2.62065506,  0.20707408, -3.04566417, -0.07017817, -0.36978954],\n",
       "       [-2.69690378, -2.69371613,  1.81732154, -2.48116849,  1.17984604],\n",
       "       [ 0.94269715,  0.514824  ,  1.2662794 ,  0.63492152, -0.31489966],\n",
       "       [-3.46522679, -3.06102946, -0.11290042, -1.55895397,  0.6221744 ],\n",
       "       [-1.60478331, -0.26251803, -2.16269167,  1.04049137, -1.36924133],\n",
       "       [-0.5391745 ,  0.93282076, -0.89758057,  0.09093895, -0.55715758],\n",
       "       [-0.61580112, -1.54912587,  0.14397594, -0.13947066, -0.09836534],\n",
       "       [-0.36871558, -0.42081446,  0.41098784,  0.80567671, -0.82416457],\n",
       "       [ 0.40409856,  0.06754632, -0.87071953,  0.66767938,  0.0696528 ],\n",
       "       [ 1.45837897, -1.11541641, -0.38346816,  0.1074973 , -1.04169341],\n",
       "       [ 0.74658666, -1.4767114 , -0.46888638,  0.61622234, -0.39949385],\n",
       "       [-0.61580112, -1.54912587,  0.14397594, -0.13947066, -0.09836534],\n",
       "       [ 1.46094712,  0.4025418 ,  1.14333103,  0.71757107, -0.09264611],\n",
       "       [ 1.33067674, -0.60803339,  0.2964346 ,  1.30263446,  0.79229197],\n",
       "       [ 1.16611326,  0.76827433,  0.34645893, -0.69689096, -0.5746567 ],\n",
       "       [ 1.37430529,  0.76800276, -0.08471199,  0.2328008 ,  1.05463035],\n",
       "       [-0.71265525,  0.48707812,  0.71407257, -0.30069909, -0.83820783],\n",
       "       [-1.65419025,  4.73549204,  0.14608848,  1.27609953,  1.38893536],\n",
       "       [ 1.2425171 , -0.65535283, -0.82189052, -0.62099645, -1.38694782],\n",
       "       [ 0.445665  ,  0.88004144,  0.56412165,  2.0923183 ,  1.94334027],\n",
       "       [ 1.13519504, -0.26119011, -0.82578077,  0.91057988,  1.7786078 ],\n",
       "       [-0.79807952, -1.24940419, -1.48611304,  1.59147532,  0.60007687],\n",
       "       [ 3.79572797, -0.39483044,  2.39581276, -1.90485791, -0.29410822],\n",
       "       [-0.99164082,  1.48489411, -1.21877452, -0.86781857,  0.91814042],\n",
       "       [-0.99164082,  1.48489411, -1.21877452, -0.86781857,  0.91814042],\n",
       "       [-0.67575666,  0.22398253,  0.85051766,  0.40496958, -1.09332502],\n",
       "       [-0.82782029, -2.26678481,  0.09722339, -1.22018998,  2.50594587],\n",
       "       [-0.11381788, -0.91931883,  0.26983775, -0.71750938,  1.45276745],\n",
       "       [ 2.48695469, -0.29149992,  0.60576966,  0.81009955, -0.05560091],\n",
       "       [ 5.43474941,  1.79056962, -3.13979275, -3.93556645,  0.6698347 ],\n",
       "       [-1.28739304,  0.70590688,  1.35085905, -0.98771846, -1.11661892],\n",
       "       [-1.20014037, -1.43375986, -1.36141443,  0.34087062, -1.0325405 ],\n",
       "       [ 1.47829849, -1.35386258, -0.5076868 ,  0.40582003, -0.86028946],\n",
       "       [ 0.50423694, -0.48910597, -1.26416275, -1.03795941, -2.20805027]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 降维  协方差矩阵 * 特征向量\n",
    "print(wine_std.shape)\n",
    "print(eigen_vectors.shape)\n",
    "wine_std.dot(eigen_vectors[:, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.36564764  0.4720026  -1.11031457 -1.00504191  0.15417471]\n",
      " [-0.05018749  0.79750082 -1.95384943  0.09032724 -0.05366988]\n",
      " [ 0.3535635   0.24401092 -1.30234563 -0.22169751  0.09752688]\n",
      " [-3.16418264 -0.97329081  1.94979796 -1.81342354 -0.57818242]\n",
      " [ 1.36564764  0.4720026  -1.11031457 -1.00504191  0.15417471]\n",
      " [ 1.22267158  0.54560843 -0.97070229 -0.95371402 -0.01696395]\n",
      " [ 0.64947792  0.00972282 -0.65375281 -0.7407173  -1.07737445]\n",
      " [ 2.01105668 -0.79434676  0.56069188 -1.35718334  0.32142611]\n",
      " [ 0.78477238 -0.77431235  0.19024306 -2.22953694  0.9573147 ]\n",
      " [-1.205167    2.03026079  1.08577236  0.93325341  0.69743525]\n",
      " [ 0.80955264 -0.21375519 -0.72273863  0.04552891 -1.17587519]\n",
      " [-1.205167    2.03026079  1.08577236  0.93325341  0.69743525]\n",
      " [ 3.03646666  0.37932868  0.03296844  1.0293463  -0.85518884]\n",
      " [-1.441007   -1.95763858 -1.14463912  1.06944519  1.27061425]\n",
      " [-4.05930022  2.15471115 -1.64611132  0.81860366 -0.96983209]\n",
      " [-4.1007975   2.17559175 -1.59842249  0.90799786 -0.8912374 ]\n",
      " [-2.62065506  0.20707408  3.04566417  0.07017817 -0.36978954]\n",
      " [-2.69690378 -2.69371613 -1.81732154  2.48116849  1.17984604]\n",
      " [ 0.94269715  0.514824   -1.2662794  -0.63492152 -0.31489966]\n",
      " [-3.46522679 -3.06102946  0.11290042  1.55895397  0.6221744 ]\n",
      " [-1.60478331 -0.26251803  2.16269167 -1.04049137 -1.36924133]\n",
      " [-0.5391745   0.93282076  0.89758057 -0.09093895 -0.55715758]\n",
      " [-0.61580112 -1.54912587 -0.14397594  0.13947066 -0.09836534]\n",
      " [-0.36871558 -0.42081446 -0.41098784 -0.80567671 -0.82416457]\n",
      " [ 0.40409856  0.06754632  0.87071953 -0.66767938  0.0696528 ]\n",
      " [ 1.45837897 -1.11541641  0.38346816 -0.1074973  -1.04169341]\n",
      " [ 0.74658666 -1.4767114   0.46888638 -0.61622234 -0.39949385]\n",
      " [-0.61580112 -1.54912587 -0.14397594  0.13947066 -0.09836534]\n",
      " [ 1.46094712  0.4025418  -1.14333103 -0.71757107 -0.09264611]\n",
      " [ 1.33067674 -0.60803339 -0.2964346  -1.30263446  0.79229197]\n",
      " [ 1.16611326  0.76827433 -0.34645893  0.69689096 -0.5746567 ]\n",
      " [ 1.37430529  0.76800276  0.08471199 -0.2328008   1.05463035]\n",
      " [-0.71265525  0.48707812 -0.71407257  0.30069909 -0.83820783]\n",
      " [-1.65419025  4.73549204 -0.14608848 -1.27609953  1.38893536]\n",
      " [ 1.2425171  -0.65535283  0.82189052  0.62099645 -1.38694782]\n",
      " [ 0.445665    0.88004144 -0.56412165 -2.0923183   1.94334027]\n",
      " [ 1.13519504 -0.26119011  0.82578077 -0.91057988  1.7786078 ]\n",
      " [-0.79807952 -1.24940419  1.48611304 -1.59147532  0.60007687]\n",
      " [ 3.79572797 -0.39483044 -2.39581276  1.90485791 -0.29410822]\n",
      " [-0.99164082  1.48489411  1.21877452  0.86781857  0.91814042]\n",
      " [-0.99164082  1.48489411  1.21877452  0.86781857  0.91814042]\n",
      " [-0.67575666  0.22398253 -0.85051766 -0.40496958 -1.09332502]\n",
      " [-0.82782029 -2.26678481 -0.09722339  1.22018998  2.50594587]\n",
      " [-0.11381788 -0.91931883 -0.26983775  0.71750938  1.45276745]\n",
      " [ 2.48695469 -0.29149992 -0.60576966 -0.81009955 -0.05560091]\n",
      " [ 5.43474941  1.79056962  3.13979275  3.93556645  0.6698347 ]\n",
      " [-1.28739304  0.70590688 -1.35085905  0.98771846 -1.11661892]\n",
      " [-1.20014037 -1.43375986  1.36141443 -0.34087062 -1.0325405 ]\n",
      " [ 1.47829849 -1.35386258  0.5076868  -0.40582003 -0.86028946]\n",
      " [ 0.50423694 -0.48910597  1.26416275  1.03795941 -2.20805027]]\n",
      "xxx\n",
      "[0.29182872 0.16210228 0.12530661 0.11511078 0.08138731]\n"
     ]
    }
   ],
   "source": [
    "# 方法二: PCA 降维\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pca = PCA(n_components=5).fit(wine_std)\n",
    "wine_pca_new = pca.transform(wine_std)\n",
    "ratio_pca = pca.explained_variance_ratio_\n",
    "print(wine_pca_new)\n",
    "print(\"xxx\")\n",
    "print(ratio_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PCA in module sklearn.decomposition.pca object:\n",
      "\n",
      "class PCA(sklearn.decomposition.base._BasePCA)\n",
      " |  PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
      " |  \n",
      " |  Principal component analysis (PCA)\n",
      " |  \n",
      " |  Linear dimensionality reduction using Singular Value Decomposition of the\n",
      " |  data to project it to a lower dimensional space.\n",
      " |  \n",
      " |  It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
      " |  SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
      " |  data and the number of components to extract.\n",
      " |  \n",
      " |  It can also use the scipy.sparse.linalg ARPACK implementation of the\n",
      " |  truncated SVD.\n",
      " |  \n",
      " |  Notice that this class does not support sparse input. See\n",
      " |  :class:`TruncatedSVD` for an alternative with sparse data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <PCA>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, float, None or string\n",
      " |      Number of components to keep.\n",
      " |      if n_components is not set all components are kept::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features)\n",
      " |  \n",
      " |      If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n",
      " |      MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n",
      " |      will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n",
      " |  \n",
      " |      If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n",
      " |      number of components such that the amount of variance that needs to be\n",
      " |      explained is greater than the percentage specified by n_components.\n",
      " |  \n",
      " |      If ``svd_solver == 'arpack'``, the number of components must be\n",
      " |      strictly less than the minimum of n_features and n_samples.\n",
      " |  \n",
      " |      Hence, the None case results in::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features) - 1\n",
      " |  \n",
      " |  copy : bool (default True)\n",
      " |      If False, data passed to fit are overwritten and running\n",
      " |      fit(X).transform(X) will not yield the expected results,\n",
      " |      use fit_transform(X) instead.\n",
      " |  \n",
      " |  whiten : bool, optional (default False)\n",
      " |      When True (False by default) the `components_` vectors are multiplied\n",
      " |      by the square root of n_samples and then divided by the singular values\n",
      " |      to ensure uncorrelated outputs with unit component-wise variances.\n",
      " |  \n",
      " |      Whitening will remove some information from the transformed signal\n",
      " |      (the relative variance scales of the components) but can sometime\n",
      " |      improve the predictive accuracy of the downstream estimators by\n",
      " |      making their data respect some hard-wired assumptions.\n",
      " |  \n",
      " |  svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n",
      " |      auto :\n",
      " |          the solver is selected by a default policy based on `X.shape` and\n",
      " |          `n_components`: if the input data is larger than 500x500 and the\n",
      " |          number of components to extract is lower than 80% of the smallest\n",
      " |          dimension of the data, then the more efficient 'randomized'\n",
      " |          method is enabled. Otherwise the exact full SVD is computed and\n",
      " |          optionally truncated afterwards.\n",
      " |      full :\n",
      " |          run exact full SVD calling the standard LAPACK solver via\n",
      " |          `scipy.linalg.svd` and select the components by postprocessing\n",
      " |      arpack :\n",
      " |          run SVD truncated to n_components calling ARPACK solver via\n",
      " |          `scipy.sparse.linalg.svds`. It requires strictly\n",
      " |          0 < n_components < min(X.shape)\n",
      " |      randomized :\n",
      " |          run randomized SVD by the method of Halko et al.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  tol : float >= 0, optional (default .0)\n",
      " |      Tolerance for singular values computed by svd_solver == 'arpack'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  iterated_power : int >= 0, or 'auto', (default 'auto')\n",
      " |      Number of iterations for the power method computed by\n",
      " |      svd_solver == 'randomized'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : array, shape (n_components, n_features)\n",
      " |      Principal axes in feature space, representing the directions of\n",
      " |      maximum variance in the data. The components are sorted by\n",
      " |      ``explained_variance_``.\n",
      " |  \n",
      " |  explained_variance_ : array, shape (n_components,)\n",
      " |      The amount of variance explained by each of the selected components.\n",
      " |  \n",
      " |      Equal to n_components largest eigenvalues\n",
      " |      of the covariance matrix of X.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  explained_variance_ratio_ : array, shape (n_components,)\n",
      " |      Percentage of variance explained by each of the selected components.\n",
      " |  \n",
      " |      If ``n_components`` is not set then all components are stored and the\n",
      " |      sum of the ratios is equal to 1.0.\n",
      " |  \n",
      " |  singular_values_ : array, shape (n_components,)\n",
      " |      The singular values corresponding to each of the selected components.\n",
      " |      The singular values are equal to the 2-norms of the ``n_components``\n",
      " |      variables in the lower-dimensional space.\n",
      " |  \n",
      " |  mean_ : array, shape (n_features,)\n",
      " |      Per-feature empirical mean, estimated from the training set.\n",
      " |  \n",
      " |      Equal to `X.mean(axis=0)`.\n",
      " |  \n",
      " |  n_components_ : int\n",
      " |      The estimated number of components. When n_components is set\n",
      " |      to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n",
      " |      number is estimated from input data. Otherwise it equals the parameter\n",
      " |      n_components, or the lesser value of n_features and n_samples\n",
      " |      if n_components is None.\n",
      " |  \n",
      " |  noise_variance_ : float\n",
      " |      The estimated noise covariance following the Probabilistic PCA model\n",
      " |      from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
      " |      Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
      " |      http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
      " |      compute the estimated data covariance and score samples.\n",
      " |  \n",
      " |      Equal to the average of (min(n_features, n_samples) - n_components)\n",
      " |      smallest eigenvalues of the covariance matrix of X.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  For n_components == 'mle', this class uses the method of `Minka, T. P.\n",
      " |  \"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604`\n",
      " |  \n",
      " |  Implements the probabilistic PCA model from:\n",
      " |  `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n",
      " |  component analysis\". Journal of the Royal Statistical Society:\n",
      " |  Series B (Statistical Methodology), 61(3), 611-622.\n",
      " |  via the score and score_samples methods.\n",
      " |  See http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |  \n",
      " |  For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n",
      " |  \n",
      " |  For svd_solver == 'randomized', see:\n",
      " |  `Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n",
      " |  \"Finding structure with randomness: Probabilistic algorithms for\n",
      " |  constructing approximate matrix decompositions\".\n",
      " |  SIAM review, 53(2), 217-288.` and also\n",
      " |  `Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n",
      " |  \"A randomized algorithm for the decomposition of matrices\".\n",
      " |  Applied and Computational Harmonic Analysis, 30(1), 47-68.`\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.decomposition import PCA\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> pca = PCA(n_components=2)\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      " |    svd_solver='auto', tol=0.0, whiten=False)\n",
      " |  >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
      " |  [0.9924... 0.0075...]\n",
      " |  >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n",
      " |  [6.30061... 0.54980...]\n",
      " |  \n",
      " |  >>> pca = PCA(n_components=2, svd_solver='full')\n",
      " |  >>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
      " |  PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      " |    svd_solver='full', tol=0.0, whiten=False)\n",
      " |  >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
      " |  [0.9924... 0.00755...]\n",
      " |  >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n",
      " |  [6.30061... 0.54980...]\n",
      " |  \n",
      " |  >>> pca = PCA(n_components=1, svd_solver='arpack')\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
      " |    svd_solver='arpack', tol=0.0, whiten=False)\n",
      " |  >>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
      " |  [0.99244...]\n",
      " |  >>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n",
      " |  [6.30061...]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  KernelPCA\n",
      " |  SparsePCA\n",
      " |  TruncatedSVD\n",
      " |  IncrementalPCA\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PCA\n",
      " |      sklearn.decomposition.base._BasePCA\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the model with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit the model with X and apply the dimensionality reduction on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Return the average log-likelihood of all samples.\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array, shape(n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : float\n",
      " |          Average log-likelihood of the samples under the current model\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Return the log-likelihood of each sample.\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array, shape(n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : array, shape (n_samples,)\n",
      " |          Log-likelihood of each sample under the current model\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.decomposition.base._BasePCA:\n",
      " |  \n",
      " |  get_covariance(self)\n",
      " |      Compute data covariance with the generative model.\n",
      " |      \n",
      " |      ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n",
      " |      where  S**2 contains the explained variances, and sigma2 contains the\n",
      " |      noise variances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cov : array, shape=(n_features, n_features)\n",
      " |          Estimated covariance of data.\n",
      " |  \n",
      " |  get_precision(self)\n",
      " |      Compute data precision matrix with the generative model.\n",
      " |      \n",
      " |      Equals the inverse of the covariance but computed with\n",
      " |      the matrix inversion lemma for efficiency.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      precision : array, shape=(n_features, n_features)\n",
      " |          Estimated precision of data.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Transform data back to its original space.\n",
      " |      \n",
      " |      In other words, return an input X_original whose transform would be X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_components)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_components is the number of components.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If whitening is enabled, inverse_transform will compute the\n",
      " |      exact inverse operation, which includes reversing whitening.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Apply dimensionality reduction to X.\n",
      " |      \n",
      " |      X is projected on the first principal components previously extracted\n",
      " |      from a training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> from sklearn.decomposition import IncrementalPCA\n",
      " |      >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |      >>> ipca = IncrementalPCA(n_components=2, batch_size=3)\n",
      " |      >>> ipca.fit(X)\n",
      " |      IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)\n",
      " |      >>> ipca.transform(X) # doctest: +SKIP\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
